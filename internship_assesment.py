# -*- coding: utf-8 -*-
"""internship assesment

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ceqwBV10ai0S8pfUqFQKj7p4YLMd2W0-
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import LinearSVC
from sklearn.metrics import classification_report, precision_score, recall_score, f1_score
from sklearn.metrics.pairwise import cosine_similarity

# Step 1: Data Preprocessing
# Load the dataset
file_path = "/content/job_description.csv"  # Adjust the path accordingly
data = pd.read_csv(file_path, on_bad_lines='skip', low_memory=False)
print(data.head())

# Check for column names to ensure correct referencing
print(data.columns)

# Clean the job descriptions (Remove stop words, punctuation, lowercase)
def clean_text(text):
    if isinstance(text, str):  # Check if the value is a string
        text = text.lower()
    return text

# Apply the cleaning function to the 'Description' column
data['cleaned_description'] = data['Description'].apply(clean_text)



# Replace NaN values in 'cleaned_description' with an empty string
data['cleaned_description'] = data['cleaned_description'].fillna('')

# Feature Extraction: Convert job descriptions into numerical vectors using TF-IDF
vectorizer = TfidfVectorizer(max_features=5000)
X = vectorizer.fit_transform(data['cleaned_description'])
y = data['Category']  # Update to use 'Category' instead of 'Categories'


# Step 2: Job Classification (SVM)
# Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train an SVM model
model = LinearSVC()
model.fit(X_train, y_train)

# Predictions
predictions = model.predict(X_test)

# Evaluate Classification Performance
print("Classification Report:\n", classification_report(y_test, predictions))

precision = precision_score(y_test, predictions, average='weighted')
recall = recall_score(y_test, predictions, average='weighted')
f1 = f1_score(y_test, predictions, average='weighted')
print(f"Precision: {precision}, Recall: {recall}, F1-Score: {f1}")

# Step 3: Job Recommendations (Content-Based Filtering)
# Assume user profile contains a list of skills (e.g., 'python, data analysis')
user_profile = "python, data analysis"  # Example user profile (should be dynamic)

# Convert user profile into a vector (using same vectorizer as job descriptions)
user_profile_vector = vectorizer.transform([user_profile])

# Calculate cosine similarity between the user profile and job descriptions
similarity_scores = cosine_similarity(user_profile_vector, X)

# Recommend top N jobs (e.g., Top 5 jobs)
recommended_jobs = similarity_scores.argsort()[0][-5:]  # Get indices of top 5 similar jobs
print("\nRecommended Jobs:", recommended_jobs)

# Step 4: Evaluation for Job Recommendations using Mean Average Precision (MAP)
def mean_average_precision(recommended, relevant):
    average_precisions = []
    for rec, rel in zip(recommended, relevant):
        hits = 0
        avg_prec = 0
        for i, r in enumerate(rec):
            if r in rel:
                hits += 1
                avg_prec += hits / (i + 1)
        average_precisions.append(avg_prec / len(rel) if rel else 0)
    return np.mean(average_precisions)

# Example of true relevant jobs (for evaluation)
# For each user, you would have a list of relevant jobs. Here it's hardcoded.
relevant_jobs = [[1, 3], [5, 7]]  # Example of relevant jobs for user profiles (replace with actual data)
recommended_jobs = [[1, 2, 3], [4, 5, 6]]  # Example of recommended jobs (replace with actual data)

# Calculate MAP for recommendations
map_score = mean_average_precision(recommended_jobs, relevant_jobs)
print(f"Mean Average Precision (MAP): {map_score}")

